import pysftp as sftp
import pandas as pd
import pandas_gbq
import time

project_id = 'your-bigquery-project-id'
host = 'yourusername'
username = 'yourusername'
password = 'yourpass'
cnopts = sftp.CnOpts()
cnopts.hostkeys = None   # this normally requires a SSH key in your known_hosts file. This line bypasses the need for that. If using PII or sensitive data, do not bypass SSH. 


sql = """
SELECT * FROM a.table"""

df = pandas_gbq.read_gbq(sql, project_id=project_id)
file_name = r'upload.csv'  #local staging, could make it elsewhere if you wanted
df.to_csv(file_name,index=False)
print ('sleeping for 2s..') #added sleep to give time for file to compile locally, may not be necessary in other use cases
time.sleep(2)
print('done sleeping..')
print(df)


def push_file_to_server():
    s = sftp.Connection(host=host, username=username, password=password, cnopts=cnopts)
    local_path = r'upload.csv'
    remote_path = "/remotestfpfolderpath/upload.csv"

    s.put(local_path, remote_path)
    s.close()

push_file_to_server()
print('done pushing new file to sftp server..')
